# app/api/news_service.py
import requests
import json
from datetime import datetime, timedelta
from typing import List, Dict, Optional
import os
from dataclasses import dataclass

# ä»é¡¹ç›®é…ç½®ä¸­å¯¼å…¥Settings
from ...config.settings import Settings
from .akshare_service import AkshareService
from .tavily_service import TavilyService

# å¯¼å…¥ç»Ÿä¸€çš„è‚¡ç¥¨ä»£ç å¤„ç†å™¨
from ..utils.symbol_processor import get_symbol_processor


# --- 1. å®šä¹‰è‡ªå®šä¹‰å¼‚å¸¸ ---
class NewsNotFoundError(Exception):
    """å½“APIè°ƒç”¨æˆåŠŸä½†æœªè¿”å›ä»»ä½•æ–°é—»æ•°æ®æ—¶å¼•å‘çš„è‡ªå®šä¹‰å¼‚å¸¸ã€‚"""

    pass


@dataclass
class NewsItem:
    """æ–°é—»é¡¹ç›®æ•°æ®ç»“æ„"""

    title: str
    content: str
    source: str
    publish_time: datetime
    url: str
    urgency: str  # high, medium, low
    relevance_score: float


class RealtimeNewsAggregator:
    """å®æ—¶æ–°é—»èšåˆå™¨"""

    def __init__(self, settings: Settings):
        self.headers = {"User-Agent": "TradingAgents-CN/1.0"}
        # ä»é…ç½®å¯¹è±¡ä¸­è·å–APIå¯†é’¥
        self.finnhub_key = settings.finnhub_api_key
        self.alpha_vantage_key = settings.alpha_vantage_api_key
        self.newsapi_key = settings.newsapi_key

        # åˆå§‹åŒ– Tavily æœåŠ¡
        self.tavily_service = TavilyService(settings)

        print("âœ… RealtimeNewsAggregator åˆå§‹åŒ–æˆåŠŸ")
        if not self.finnhub_key:
            print("âš ï¸ FINNHUB_API_KEY æœªé…ç½®")
        if not self.tavily_service.is_available():
            print("âš ï¸ Tavily æœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥ï¼Œå°†è·³è¿‡æ·±åº¦ç ”ç©¶")

    # --- 2. ä¿®æ”¹æ ¸å¿ƒæ•°æ®è·å–å‡½æ•°ï¼Œä½¿å…¶åœ¨æ— æ•°æ®æ—¶æŠ›å‡ºå¼‚å¸¸ ---
    def get_realtime_stock_news(
        self, ticker: str, days_back: int = 30
    ) -> List[NewsItem]:
        """
        èšåˆæ‰€æœ‰æ¥æºçš„å®æ—¶è‚¡ç¥¨æ–°é—»ã€‚
        å¦‚æœæ‰€æœ‰æ¥æºéƒ½æ²¡æœ‰æ•°æ®ï¼Œåˆ™æŠ›å‡º NewsNotFoundErrorã€‚
        """
        # å¯¹è‚¡ç¥¨ä»£ç è¿›è¡Œæ ‡å‡†åŒ–å¤„ç†
        processor = get_symbol_processor()
        symbol_info = processor.process_symbol(ticker)

        stock_info = symbol_info  # ä¿æŒå…¼å®¹æ€§
        standardized_ticker = symbol_info["formats"]["news_api"]

        print(f"ğŸ” [æ–°é—»èšåˆ] åŸå§‹ä»£ç : {ticker} -> æ ‡å‡†åŒ–ä»£ç : {standardized_ticker}")
        print(
            f"ğŸ“Š [è‚¡ç¥¨åˆ†æ] {ticker} -> {symbol_info['market']} | {symbol_info['exchange']} | {symbol_info['board']}"
        )

        all_news = []

        # 1. FinnHubå®æ—¶æ–°é—» (å¦‚æœé…ç½®äº†Key)
        if self.finnhub_key:
            try:
                # _get_finnhub_realtime_news å†…éƒ¨ä¸æŠ›å‡ºâ€œæœªæ‰¾åˆ°â€å¼‚å¸¸ï¼Œä»…è¿”å›åˆ—è¡¨
                # èšåˆå™¨è´Ÿè´£åˆ¤æ–­æœ€ç»ˆç»“æœ
                all_news.extend(
                    self._get_finnhub_realtime_news(standardized_ticker, days_back)
                )
            except Exception as e:
                # æ•è·APIè¿æ¥ç­‰å…¶ä»–é”™è¯¯
                print(f"âŒ è°ƒç”¨FinnHubæ–°é—»æºæ—¶å‡ºé”™: {e}")

        # 2. Alpha Vantageæ–°é—» (å¦‚æœé…ç½®äº†Key)
        if self.alpha_vantage_key:
            try:
                all_news.extend(
                    self._get_alpha_vantage_news(standardized_ticker, days_back)
                )
            except Exception as e:
                print(f"âŒ è°ƒç”¨Alpha Vantageæ–°é—»æºæ—¶å‡ºé”™: {e}")

        # 3. NewsAPIæ–°é—» (å¦‚æœé…ç½®äº†Key)
        if self.newsapi_key:
            try:
                all_news.extend(self._get_newsapi_news(standardized_ticker, days_back))
            except Exception as e:
                print(f"âŒ è°ƒç”¨NewsAPIæ–°é—»æºæ—¶å‡ºé”™: {e}")

        # 4. ä¸­æ–‡è´¢ç»æ–°é—»æº (åŸºäºdataflowsä¸­çš„å®ç°)
        try:
            all_news.extend(self._get_chinese_finance_news(ticker, days_back))
        except Exception as e:
            print(f"âŒ è°ƒç”¨ä¸­æ–‡è´¢ç»æ–°é—»æºæ—¶å‡ºé”™: {e}")

        # 5. Tavily æ·±åº¦ç½‘ç»œæœç´¢ (ä½œä¸ºè¡¥å……)
        if self.tavily_service.is_available():
            try:
                query = f"å…³äº {symbol_info.get('name', ticker)} ({ticker}) çš„æœ€æ–°å¸‚åœºåˆ†æã€æ–°é—»å’Œæ·±åº¦è§è§£"
                tavily_news = self._get_tavily_research_as_news(query)
                all_news.extend(tavily_news)
            except Exception as e:
                print(f"âŒ è°ƒç”¨ Tavily ç ”ç©¶æ—¶å‡ºé”™: {e}")

        # --- æ ¸å¿ƒæ”¹åŠ¨ ---
        # åœ¨å°è¯•æ‰€æœ‰æ–°é—»æºåï¼Œå¦‚æœåˆ—è¡¨ä»ä¸ºç©ºï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
        if not all_news:
            raise NewsNotFoundError(
                f"æœªèƒ½ä»ä»»ä½•é…ç½®çš„æ–°é—»æºè·å–åˆ°å…³äº {ticker} çš„æ–°é—»ã€‚"
            )

        unique_news = self._deduplicate_news(all_news)
        return sorted(unique_news, key=lambda x: x.publish_time, reverse=True)

    def _get_tavily_research_as_news(self, query: str) -> List[NewsItem]:
        """ä½¿ç”¨ Tavily è¿›è¡Œæ·±åº¦ç ”ç©¶ï¼Œå¹¶å°†ç»“æœè½¬æ¢ä¸º NewsItem æ ¼å¼"""
        search_result = self.tavily_service.search(
            query, search_depth="basic", max_results=5
        )
        if not search_result or not search_result.get("results"):
            return []

        news_items = []
        # é¦–å…ˆï¼Œå°† Tavily çš„ AI æ€»ç»“ç­”æ¡ˆä½œä¸ºä¸€ä¸ªç‰¹æ®Šçš„æ–°é—»é¡¹
        if search_result.get("answer"):
            news_items.append(
                NewsItem(
                    title="[Tavily AI æ€»ç»“]",
                    content=search_result["answer"],
                    source="Tavily AI",
                    publish_time=datetime.now(),
                    url="",
                    urgency="high",
                    relevance_score=1.0,
                )
            )

        # ç„¶åï¼Œå¤„ç†å…·ä½“çš„æœç´¢ç»“æœ
        for item in search_result["results"]:
            news_items.append(
                NewsItem(
                    title=item.get("title", ""),
                    content=item.get("content", ""),
                    source=item.get("source", "Tavily"),
                    publish_time=datetime.now(),  # Tavilyä¸æä¾›å‘å¸ƒæ—¶é—´ï¼Œä½¿ç”¨å½“å‰æ—¶é—´
                    url=item.get("url", ""),
                    urgency="medium",
                    relevance_score=item.get("score", 0.8),
                )
            )
        print(f"âœ… [Tavily] è·å–åˆ° {len(news_items)} æ¡ç ”ç©¶ç»“æœ")
        return news_items

    def _get_standardized_ticker_for_news(self, ticker: str, stock_info: dict) -> str:
        """
        ä¸ºæ–°é—»APIæ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç 

        Args:
            ticker: åŸå§‹è‚¡ç¥¨ä»£ç 
            stock_info: è‚¡ç¥¨åˆ†ç±»ä¿¡æ¯

        Returns:
            str: æ ‡å‡†åŒ–åçš„è‚¡ç¥¨ä»£ç 
        """
        # ä½¿ç”¨ç»Ÿä¸€çš„ä»£ç å¤„ç†å™¨
        processor = get_symbol_processor()
        return processor.get_news_api_format(ticker)

    def _get_finnhub_realtime_news(self, ticker: str, days_back: int) -> List[NewsItem]:
        """
        ä»FinnHubè·å–å®æ—¶æ–°é—»ã€‚
        æ­¤å‡½æ•°åœ¨APIè°ƒç”¨å¤±è´¥æ—¶æ‰“å°é”™è¯¯å¹¶è¿”å›ç©ºåˆ—è¡¨ï¼ŒæˆåŠŸä½†æ— æ•°æ®æ—¶ä¹Ÿè¿”å›ç©ºåˆ—è¡¨ã€‚
        ç”±ä¸Šå±‚èšåˆå™¨å†³å®šæ˜¯å¦æŠ›å‡ºå¼‚å¸¸ã€‚
        """
        if not self.finnhub_key:
            return []

        print(f"ğŸ” [FinnHub] æ­£åœ¨è·å– {ticker} çš„æ–°é—»...")
        try:
            end_time = datetime.now()
            start_time = end_time - timedelta(days=days_back)

            url = "https://finnhub.io/api/v1/company-news"
            params = {
                "symbol": ticker,
                "from": start_time.strftime("%Y-%m-%d"),
                "to": end_time.strftime("%Y-%m-%d"),
                "token": self.finnhub_key,
            }

            response = requests.get(
                url, params=params, headers=self.headers, timeout=10
            )
            response.raise_for_status()  # å¦‚æœå‘ç”ŸHTTPé”™è¯¯ (å¦‚ 401, 403, 500), åˆ™ä¼šæŠ›å‡ºå¼‚å¸¸

            news_data = response.json()
            # å¦‚æœ news_data æ˜¯ç©ºåˆ—è¡¨ï¼Œç›´æ¥è¿”å›ç©ºåˆ—è¡¨ï¼Œè®©è°ƒç”¨è€…å¤„ç†
            if not news_data:
                print(f"â„¹ï¸ [FinnHub] æœªæ‰¾åˆ°å…³äº {ticker} çš„æ–°é—»ã€‚")
                return []

            news_items = []
            for item in news_data:
                publish_time = datetime.fromtimestamp(item.get("datetime", 0))
                if publish_time < start_time:
                    continue

                news_items.append(
                    NewsItem(
                        title=item.get("headline", ""),
                        content=item.get("summary", ""),
                        source="FinnHub",
                        publish_time=publish_time,
                        url=item.get("url", ""),
                        urgency=self._assess_news_urgency(
                            item.get("headline", ""), item.get("summary", "")
                        ),
                        relevance_score=self._calculate_relevance(
                            item.get("headline", ""), ticker
                        ),
                    )
                )
            print(f"âœ… [FinnHub] è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items

        except requests.exceptions.RequestException as e:
            # æ•è·ç½‘ç»œæˆ–HTTPé”™è¯¯
            print(f"âŒ [FinnHub] æ–°é—»è·å–å¤±è´¥ (ç½‘ç»œ/HTTPé”™è¯¯): {e}")
            return []
        except Exception as e:
            # æ•è·å…¶ä»–æ‰€æœ‰æœªçŸ¥é”™è¯¯
            print(f"âŒ [FinnHub] æ–°é—»è·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _assess_news_urgency(self, title: str, content: str) -> str:
        """ç®€å•è¯„ä¼°æ–°é—»ç´§æ€¥ç¨‹åº¦"""
        text = (title + " " + content).lower()
        high_urgency = [
            "breaking",
            "alert",
            "urgent",
            "halt",
            "suspend",
            "investigation",
            "lawsuit",
            "warning",
            "çªå‘",
            "ç´§æ€¥",
            "æš‚åœ",
            "é‡å¤§",
            "è°ƒæŸ¥",
            "è¯‰è®¼",
            "è­¦å‘Š",
            "è¿è§„",
        ]
        medium_urgency = [
            "earnings",
            "report",
            "announce",
            "merger",
            "acquisition",
            "partnership",
            "guidance",
            "outlook",
            "new product",
            "launch",
            "è´¢æŠ¥",
            "å‘å¸ƒ",
            "å®£å¸ƒ",
            "å¹¶è´­",
            "åˆä½œ",
            "æ–°å“",
            "æŒ‡å¼•",
            "å±•æœ›",
        ]

        if any(k in text for k in high_urgency):
            return "high"
        if any(k in text for k in medium_urgency):
            return "medium"
        return "low"

    def _calculate_relevance(self, title: str, ticker: str) -> float:
        """ç®€å•è®¡ç®—æ–°é—»ç›¸å…³æ€§"""
        return 1.0 if ticker.lower() in title.lower() else 0.5

    def _deduplicate_news(self, news_items: List[NewsItem]) -> List[NewsItem]:
        """åŸºäºæ–°é—»æ ‡é¢˜è¿›è¡Œå»é‡"""
        seen_titles = set()
        unique_news = []
        for item in news_items:
            title_key = item.title.lower().strip()
            if title_key and title_key not in seen_titles:
                seen_titles.add(title_key)
                unique_news.append(item)
        return unique_news

    def _get_alpha_vantage_news(self, ticker: str, days_back: int) -> List[NewsItem]:
        """ä»Alpha Vantageè·å–æ–°é—»"""
        if not self.alpha_vantage_key:
            return []

        print(f"ğŸ” [Alpha Vantage] æ­£åœ¨è·å– {ticker} çš„æ–°é—»...")
        try:
            url = "https://www.alphavantage.co/query"
            params = {
                "function": "NEWS_SENTIMENT",
                "tickers": ticker,
                "apikey": self.alpha_vantage_key,
                "limit": 50,
            }

            response = requests.get(
                url, params=params, headers=self.headers, timeout=10
            )
            response.raise_for_status()

            data = response.json()
            news_items = []

            if "feed" in data:
                start_time = datetime.now() - timedelta(days=days_back)

                for item in data["feed"]:
                    # è§£ææ—¶é—´
                    time_str = item.get("time_published", "")
                    try:
                        publish_time = datetime.strptime(time_str, "%Y%m%dT%H%M%S")
                    except:
                        continue

                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < start_time:
                        continue

                    news_items.append(
                        NewsItem(
                            title=item.get("title", ""),
                            content=item.get("summary", ""),
                            source="Alpha Vantage",
                            publish_time=publish_time,
                            url=item.get("url", ""),
                            urgency=self._assess_news_urgency(
                                item.get("title", ""), item.get("summary", "")
                            ),
                            relevance_score=self._calculate_relevance(
                                item.get("title", ""), ticker
                            ),
                        )
                    )

            print(f"âœ… [Alpha Vantage] è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items

        except requests.exceptions.RequestException as e:
            print(f"âŒ [Alpha Vantage] æ–°é—»è·å–å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âŒ [Alpha Vantage] æ–°é—»è·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _get_newsapi_news(self, ticker: str, days_back: int) -> List[NewsItem]:
        """ä»NewsAPIè·å–æ–°é—»"""
        if not self.newsapi_key:
            return []

        print(f"ğŸ” [NewsAPI] æ­£åœ¨è·å– {ticker} çš„æ–°é—»...")
        try:
            # æ„å»ºæœç´¢æŸ¥è¯¢
            company_names = {
                "AAPL": "Apple",
                "TSLA": "Tesla",
                "NVDA": "NVIDIA",
                "MSFT": "Microsoft",
                "GOOGL": "Google",
            }

            query = f"{ticker} OR {company_names.get(ticker, ticker)}"

            url = "https://newsapi.org/v2/everything"
            params = {
                "q": query,
                "language": "en",
                "sortBy": "publishedAt",
                "from": (datetime.now() - timedelta(days=days_back)).isoformat(),
                "apiKey": self.newsapi_key,
            }

            response = requests.get(
                url, params=params, headers=self.headers, timeout=10
            )
            response.raise_for_status()

            data = response.json()
            news_items = []

            for item in data.get("articles", []):
                # è§£ææ—¶é—´
                time_str = item.get("publishedAt", "")
                try:
                    publish_time = datetime.fromisoformat(
                        time_str.replace("Z", "+00:00")
                    )
                    # ç§»é™¤æ—¶åŒºä¿¡æ¯ï¼Œç»Ÿä¸€ä¸ºnaive datetime
                    publish_time = publish_time.replace(tzinfo=None)
                except:
                    continue

                news_items.append(
                    NewsItem(
                        title=item.get("title", ""),
                        content=item.get("description", ""),
                        source=item.get("source", {}).get("name", "NewsAPI"),
                        publish_time=publish_time,
                        url=item.get("url", ""),
                        urgency=self._assess_news_urgency(
                            item.get("title", ""), item.get("description", "")
                        ),
                        relevance_score=self._calculate_relevance(
                            item.get("title", ""), ticker
                        ),
                    )
                )

            print(f"âœ… [NewsAPI] è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
            return news_items

        except requests.exceptions.RequestException as e:
            print(f"âŒ [NewsAPI] æ–°é—»è·å–å¤±è´¥: {e}")
            return []
        except Exception as e:
            print(f"âŒ [NewsAPI] æ–°é—»è·å–æ—¶å‘ç”ŸæœªçŸ¥é”™è¯¯: {e}")
            return []

    def _get_chinese_finance_news(self, ticker: str, days_back: int) -> List[NewsItem]:
        """è·å–ä¸­æ–‡è´¢ç»æ–°é—»ï¼ŒåŸºäºdataflowsä¸­çš„å®ç°"""
        print(f"ğŸ” [ä¸­æ–‡è´¢ç»] æ­£åœ¨è·å– {ticker} çš„ä¸­æ–‡è´¢ç»æ–°é—»...")
        news_items = []

        # ä½¿ç”¨ç»Ÿä¸€çš„è‚¡ç¥¨åˆ†ç±»å™¨åˆ¤æ–­å¸‚åœºç±»å‹
        processor = get_symbol_processor()
        symbol_info = processor.process_symbol(ticker)

        stock_info = symbol_info  # ä¿æŒå…¼å®¹æ€§

        print(
            f"ğŸ“Š [è‚¡ç¥¨åˆ†æ] {ticker} -> {symbol_info['market']} | {symbol_info['exchange']} | {symbol_info['board']}"
        )

        # 1. å°è¯•ä½¿ç”¨ä¸œæ–¹è´¢å¯Œæ–°é—»æº (æ”¯æŒAè‚¡å’Œéƒ¨åˆ†æ¸¯è‚¡)
        try:
            # Aè‚¡æˆ–æ¸¯è‚¡éƒ½å¯ä»¥å°è¯•ä¸œæ–¹è´¢å¯Œ
            if stock_info["is_china"] or stock_info["is_hk"]:
                # å¤„ç†è‚¡ç¥¨ä»£ç æ ¼å¼
                clean_ticker = self._normalize_ticker_for_eastmoney(ticker, stock_info)

                # ä½¿ç”¨ AkshareService è·å–ä¸œæ–¹è´¢å¯Œä¸ªè‚¡æ–°é—»
                try:
                    ak_service = AkshareService()
                    df = ak_service.get_stock_news_em(clean_ticker, max_news=20)
                    if df is not None and not df.empty:
                        start_time = datetime.now() - timedelta(days=days_back)

                        # å…¼å®¹å¤šç§åˆ—å
                        def _get_val(row, keys, default=""):
                            for k in keys:
                                if k in row and pd.notna(row.get(k)):
                                    return row.get(k)
                            return default

                        # æƒ°æ€§å¯¼å…¥ pandas ç”¨äºæ—¶é—´åˆ¤æ–­ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                        try:
                            import pandas as pd  # type: ignore
                            from pandas import Timestamp  # type: ignore
                        except Exception:
                            pd = None  # type: ignore
                            Timestamp = None  # type: ignore

                        for _, row in df.iterrows():
                            # æ ‡é¢˜/å†…å®¹/é“¾æ¥
                            title = _get_val(row, ["æ ‡é¢˜", "æ–°é—»æ ‡é¢˜", "title"]) or ""
                            content = (
                                _get_val(row, ["å†…å®¹", "æ–°é—»å†…å®¹", "æ‘˜è¦", "summary"])
                                or ""
                            )
                            url = _get_val(row, ["é“¾æ¥", "æ–°é—»é“¾æ¥", "url"]) or ""

                            # å‘å¸ƒæ—¶é—´è§£æ
                            time_raw = _get_val(
                                row,
                                ["æ—¶é—´", "å‘å¸ƒæ—¶é—´", "æ—¥æœŸ", "date", "publish_time"],
                                "",
                            )
                            publish_time = None
                            try:
                                if isinstance(time_raw, datetime):
                                    publish_time = time_raw
                                elif Timestamp is not None and isinstance(time_raw, Timestamp):  # type: ignore
                                    publish_time = time_raw.to_pydatetime()
                                elif isinstance(time_raw, str) and time_raw:
                                    for fmt in (
                                        "%Y-%m-%d %H:%M:%S",
                                        "%Y-%m-%d %H:%M",
                                        "%Y-%m-%d",
                                    ):
                                        try:
                                            publish_time = datetime.strptime(
                                                time_raw, fmt
                                            )
                                            break
                                        except Exception:
                                            continue
                            except Exception:
                                publish_time = None

                            # æ— æ³•è§£ææ—¶é—´åˆ™è·³è¿‡ï¼Œæˆ–ä¸è¿‡æ»¤æ—¶æ•ˆ
                            if publish_time is not None and publish_time < start_time:
                                continue

                            news_items.append(
                                NewsItem(
                                    title=title,
                                    content=content,
                                    source="ä¸œæ–¹è´¢å¯Œ",
                                    publish_time=publish_time or datetime.now(),
                                    url=url,
                                    urgency=self._assess_news_urgency(title, content),
                                    relevance_score=self._calculate_relevance(
                                        title, ticker
                                    ),
                                )
                            )
                        print(f"âœ… [ä¸­æ–‡è´¢ç»Â·ä¸œæ–¹è´¢å¯Œ] è·å–åˆ° {len(news_items)} æ¡æ–°é—»")
                except Exception as e:
                    print(
                        f"âŒ [ä¸­æ–‡è´¢ç»] è°ƒç”¨ AkshareService è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»å¤±è´¥: {e}"
                    )

        except Exception as e:
            print(f"âŒ [ä¸­æ–‡è´¢ç»] ä¸œæ–¹è´¢å¯Œæ–°é—»è·å–å¤±è´¥: {e}")

        # 2. è´¢è”ç¤¾ç­‰RSSæº (å¯¹æ‰€æœ‰å¸‚åœºéƒ½å°è¯•)
        try:
            rss_news = self._get_chinese_rss_news(ticker, days_back)
            news_items.extend(rss_news)
        except Exception as e:
            print(f"âŒ [ä¸­æ–‡è´¢ç»] RSSæ–°é—»è·å–å¤±è´¥: {e}")

        print(f"âœ… [ä¸­æ–‡è´¢ç»] è·å–åˆ° {len(news_items)} æ¡ä¸­æ–‡æ–°é—»")
        return news_items

    def _normalize_ticker_for_eastmoney(self, ticker: str, stock_info: dict) -> str:
        """ä¸ºä¸œæ–¹è´¢å¯ŒAPIæ ‡å‡†åŒ–è‚¡ç¥¨ä»£ç """
        if stock_info["is_china"]:
            # Aè‚¡ï¼šç§»é™¤åç¼€
            return (
                ticker.replace(".SH", "")
                .replace(".SZ", "")
                .replace(".SS", "")
                .replace(".XSHE", "")
                .replace(".XSHG", "")
            )
        elif stock_info["is_hk"]:
            # æ¸¯è‚¡ï¼šä½¿ç”¨5ä½æ•°å­—æ ¼å¼
            clean_code = ticker.replace(".HK", "").replace(".hk", "")
            if clean_code.isdigit():
                return clean_code.zfill(5)
            return clean_code
        else:
            # å…¶ä»–å¸‚åœºï¼šè¿”å›åŸå§‹ä»£ç 
            return ticker

    def _is_china_stock(self, ticker: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºä¸­å›½è‚¡ç¥¨"""
        from ..utils.stock_market_classifier import is_china_stock

        return is_china_stock(ticker)

    def _get_eastmoney_news(self, ticker: str, days_back: int) -> List[NewsItem]:
        """è·å–ä¸œæ–¹è´¢å¯Œæ–°é—»ï¼ˆæ¨¡æ‹Ÿå®ç°ï¼‰"""
        # å®é™…åº”è¯¥é›†æˆdataflowsä¸­çš„akshare_utils.get_stock_news_em
        # è¿™é‡Œè¿”å›æ¨¡æ‹Ÿæ•°æ®
        return []

    def _get_chinese_rss_news(self, ticker: str, days_back: int) -> List[NewsItem]:
        """è·å–ä¸­æ–‡RSSæ–°é—»æº"""
        news_items = []

        # RSSæºåˆ—è¡¨ï¼ˆå¯ä»¥æ·»åŠ æ›´å¤šè´¢ç»æ–°é—»RSSï¼‰
        rss_sources = [
            "http://feed.sina.com.cn/finance/roll/index.xml",
            "http://rss.cnstock.com/news/jgsz.xml",
        ]

        for rss_url in rss_sources:
            try:
                items = self._parse_rss_feed(rss_url, ticker, days_back)
                news_items.extend(items)
            except Exception as e:
                print(f"âŒ è§£æRSSæºå¤±è´¥ {rss_url}: {e}")

        return news_items

    def _parse_rss_feed(
        self, rss_url: str, ticker: str, days_back: int
    ) -> List[NewsItem]:
        """è§£æRSSæº"""
        try:
            import feedparser

            feed = feedparser.parse(rss_url)
            news_items = []

            if not feed or not feed.entries:
                return []

            start_time = datetime.now() - timedelta(days=days_back)

            for entry in feed.entries:
                try:
                    # è§£ææ—¶é—´
                    if hasattr(entry, "published_parsed") and entry.published_parsed:
                        import time

                        publish_time = datetime.fromtimestamp(
                            time.mktime(entry.published_parsed)
                        )
                    else:
                        continue

                    # æ£€æŸ¥æ—¶æ•ˆæ€§
                    if publish_time < start_time:
                        continue

                    title = entry.title if hasattr(entry, "title") else ""
                    content = entry.description if hasattr(entry, "description") else ""

                    # æ£€æŸ¥ç›¸å…³æ€§
                    if (
                        ticker.lower() not in title.lower()
                        and ticker.lower() not in content.lower()
                    ):
                        continue

                    news_items.append(
                        NewsItem(
                            title=title,
                            content=content,
                            source="è´¢ç»RSS",
                            publish_time=publish_time,
                            url=entry.link if hasattr(entry, "link") else "",
                            urgency=self._assess_news_urgency(title, content),
                            relevance_score=self._calculate_relevance(title, ticker),
                        )
                    )

                except Exception as e:
                    continue

            return news_items

        except ImportError:
            print("âš ï¸ feedparseråº“æœªå®‰è£…ï¼Œæ— æ³•è§£æRSSæº")
            return []
        except Exception as e:
            print(f"âŒ RSSè§£æå¤±è´¥: {e}")
            return []

    # --- 3. æŠ¥å‘Šç”Ÿæˆå‡½æ•°ä¿æŒä¸å˜ï¼Œå®ƒåªè´Ÿè´£æ ¼å¼åŒ– ---
    def format_news_report(self, news_items: List[NewsItem], ticker: str) -> str:
        """å°†æ–°é—»åˆ—è¡¨æ ¼å¼åŒ–ä¸ºMarkdownæŠ¥å‘Š"""
        if not news_items:
            # è¿™ä¸ªåˆ¤æ–­ä»ç„¶æœ‰ç”¨ï¼Œä½œä¸ºä¸€é“é˜²çº¿ï¼Œæˆ–è€…åœ¨ä¸å¸Œæœ›æŠ›å‡ºå¼‚å¸¸çš„åœºæ™¯ä¸‹ç›´æ¥è°ƒç”¨
            return f"âŒ æœªè·å–åˆ°å…³äº {ticker} çš„å®æ—¶æ–°é—»æ•°æ®ã€‚è¯·æ£€æŸ¥APIå¯†é’¥æˆ–ç¨åå†è¯•ã€‚"

        high = [n for n in news_items if n.urgency == "high"]
        medium = [n for n in news_items if n.urgency == "medium"]

        report = f"# {ticker} å®æ—¶æ–°é—»åˆ†ææŠ¥å‘Š\n\n"
        report += f"ğŸ“… ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n"
        report += f"ğŸ“Š æ–°é—»æ€»æ•° (å»é‡å): {len(news_items)}æ¡\n\n"

        if high:
            report += "## ğŸš¨ ç´§æ€¥æ–°é—»\n\n"
            for news in high[:3]:  # æœ€å¤š3æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"> {news.content}\n\n[é˜…è¯»åŸæ–‡]({news.url})\n\n"

        if medium:
            report += "## ğŸ“¢ é‡è¦æ–°é—»\n\n"
            for news in medium[:5]:  # æœ€å¤š5æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"> {news.content}\n\n[é˜…è¯»åŸæ–‡]({news.url})\n\n"

        # æ·»åŠ ä½ä¼˜å…ˆçº§æ–°é—»æ˜¾ç¤º
        low = [n for n in news_items if n.urgency == "low"]
        if low and not high and not medium:
            # å¦‚æœæ²¡æœ‰é«˜ä¼˜å…ˆçº§å’Œä¸­ä¼˜å…ˆçº§æ–°é—»ï¼Œæ˜¾ç¤ºéƒ¨åˆ†ä½ä¼˜å…ˆçº§æ–°é—»
            report += "## ğŸ“° ä¸€èˆ¬æ–°é—»\n\n"
            for news in low[:8]:  # æœ€å¤š8æ¡
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"> {news.content}\n\n[é˜…è¯»åŸæ–‡]({news.url})\n\n"
        elif low and (high or medium):
            # å¦‚æœæœ‰é«˜ä¼˜å…ˆçº§æ–°é—»ï¼Œåªæ˜¾ç¤ºæœ€æ–°çš„3æ¡ä½ä¼˜å…ˆçº§æ–°é—»
            report += "## ğŸ“° å…¶ä»–æ–°é—»\n\n"
            for news in low[:3]:
                report += f"### {news.title}\n"
                report += f"**æ¥æº**: {news.source} | **æ—¶é—´**: {news.publish_time.strftime('%H:%M')}\n"
                report += f"> {news.content}\n\n[é˜…è¯»åŸæ–‡]({news.url})\n\n"

        latest_news_time = max(n.publish_time for n in news_items)

        # å¤„ç†æ—¶åŒºé—®é¢˜ï¼šå°†æ‰€æœ‰æ—¶é—´è½¬æ¢ä¸ºnaive datetime
        current_time = datetime.now()
        if latest_news_time.tzinfo is not None:
            # å¦‚æœæ–°é—»æ—¶é—´æœ‰æ—¶åŒºä¿¡æ¯ï¼Œè½¬æ¢ä¸ºæœ¬åœ°æ—¶é—´å¹¶ç§»é™¤æ—¶åŒºä¿¡æ¯
            latest_news_time = latest_news_time.replace(tzinfo=None)

        time_diff_minutes = (current_time - latest_news_time).total_seconds() / 60

        report += f"\n## â° æ•°æ®æ—¶æ•ˆæ€§\n"
        report += f"æœ€æ–°æ–°é—»å‘å¸ƒäº: **{time_diff_minutes:.0f}åˆ†é’Ÿå‰**\n"

        if time_diff_minutes < 30:
            report += "ğŸŸ¢ æ—¶æ•ˆæ€§: ä¼˜ç§€\n"
        elif time_diff_minutes < 60:
            report += "ğŸŸ¡ æ—¶æ•ˆæ€§: è‰¯å¥½\n"
        else:
            report += "ğŸ”´ æ—¶æ•ˆæ€§: ä¸€èˆ¬\n"

        return report
